{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fa3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Procesando: Hechos de TrÃ¡nsito...\n",
      "Columna fecha detectada: 'Ã¯Â»Â¿fecha_evento'\n",
      "Hechos cargados: 132065 filas.\n",
      "\n",
      "ðŸš§ [2/4] Procesando: Incidentes Viales (Inviales)...\n",
      "Columna fecha detectada: 'fecha_creacion'\n",
      "Inviales cargados: 11559010 filas.\n",
      "\n",
      "[3/4] Procesando: Afluencia Metro...\n",
      "Columna fecha detectada: 'Ã¯Â»Â¿fecha'\n",
      "Metro cargado: 5139574 filas.\n",
      "\n",
      "[4/4] Procesando: Pluviales...\n",
      "Interpolando 265 estaciones...\n",
      "Pluviales cargados: 433747 registros.\n",
      "\n",
      "Refrescando Vista Maestra...\n",
      "Vista actualizada. Registros listos: 53439\n",
      "\n",
      "INGESTA TOTAL COMPLETADA.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib.parse\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Silenciar advertencias\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 1. CONEXIÃ“N\n",
    "USER = \"postgres\"\n",
    "PASSWORD = \"as52\"\n",
    "HOST = \"localhost\"\n",
    "PORT = \"5432\"\n",
    "DB_NAME = \"Historico_Hechos_Movilidad\"\n",
    "\n",
    "password_safe = urllib.parse.quote_plus(PASSWORD)\n",
    "user_safe = urllib.parse.quote_plus(USER)\n",
    "db_connection_str = f'postgresql+psycopg2://{user_safe}:{password_safe}@{HOST}:{PORT}/{DB_NAME}'\n",
    "engine = create_engine(db_connection_str)\n",
    "\n",
    "# FUNCIÃ“N AUXILIAR PARA ENCONTRAR COLUMNAS CON BOM\n",
    "def buscar_columna_flexible(columnas, objetivo):\n",
    "    \"\"\"Busca una columna que contenga la palabra objetivo, ignorando basura (BOM).\"\"\"\n",
    "    for col in columnas:\n",
    "        if objetivo in col:\n",
    "            return col\n",
    "    return objetivo # Devuelve el original si no encuentra match (para que falle con nombre claro)\n",
    "\n",
    "# 2. FUNCIONES DE CARGA\n",
    "\n",
    "def carga_hechos_transito():\n",
    "    print(\"\\n[1/4] Procesando: Hechos de TrÃ¡nsito...\")\n",
    "    ruta = r\"Data_Tratado\\Hechos_Transito_Enriquecido.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(ruta, encoding='latin-1', low_memory=False)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        \n",
    "        # BÃºsqueda flexible de la columna fecha\n",
    "        col_fecha = buscar_columna_flexible(df.columns, 'fecha_evento')\n",
    "        print(f\"Columna fecha detectada: '{col_fecha}'\")\n",
    "        \n",
    "        fechas = pd.to_datetime(df[col_fecha], dayfirst=True, errors='coerce')\n",
    "        df['fecha_evento'] = fechas\n",
    "        df['fk_tiempo'] = fechas\n",
    "        \n",
    "        # GeometrÃ­a\n",
    "        if 'longitud' in df.columns and 'latitud' in df.columns:\n",
    "            mask = df['longitud'].notnull() & df['latitud'].notnull()\n",
    "            df.loc[mask, 'geometria'] = df[mask].apply(\n",
    "                lambda x: f\"SRID=4326;POINT({x['longitud']} {x['latitud']})\", axis=1\n",
    "            )\n",
    "        \n",
    "        # Mapeo y Filtrado\n",
    "        cols_bd = [\n",
    "            'fecha_evento', 'hora_evento', 'tipo_evento', 'alcaldia', 'colonia', \n",
    "            'latitud', 'longitud', 'geometria', 'score_atlas', 'flag_historico', \n",
    "            'nivel_riesgo_estatico', 'fk_tiempo'\n",
    "        ]\n",
    "        df_final = df[[c for c in cols_bd if c in df.columns]].copy()\n",
    "        \n",
    "        df_final.to_sql('fact_hechos_transito', engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "        print(f\"Hechos cargados: {len(df_final)} filas.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en Hechos: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def carga_inviales():\n",
    "    print(\"\\n[2/4] Procesando: Incidentes Viales (Inviales)...\")\n",
    "    ruta = r\"Data_Tratado\\Inviales_Enriquecido.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(ruta, encoding='latin-1', low_memory=False)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        \n",
    "        # BÃºsqueda flexible\n",
    "        col_fecha = buscar_columna_flexible(df.columns, 'fecha_creacion')\n",
    "        print(f\"Columna fecha detectada: '{col_fecha}'\")\n",
    "\n",
    "        fechas = pd.to_datetime(df[col_fecha], dayfirst=True, errors='coerce')\n",
    "        df['fecha_creacion'] = fechas\n",
    "        df['fk_tiempo'] = fechas\n",
    "        \n",
    "        # GeometrÃ­a\n",
    "        mask = df['longitud'].notnull() & df['latitud'].notnull()\n",
    "        df.loc[mask, 'geometria'] = df[mask].apply(\n",
    "            lambda x: f\"SRID=4326;POINT({x['longitud']} {x['latitud']})\", axis=1\n",
    "        )\n",
    "        \n",
    "        # Renombrado seguro\n",
    "        cols_map = {\n",
    "            'tipo_incidente': 'tipo_incidente',\n",
    "            'alcaldia': 'alcaldia',\n",
    "            'hora_creacion': 'hora_creacion',\n",
    "            'score_atlas': 'score_atlas', \n",
    "            'flag_historico': 'flag_historico', \n",
    "            'nivel_riesgo_estatico': 'nivel_riesgo_estatico'\n",
    "        }\n",
    "        df = df.rename(columns=cols_map)\n",
    "        \n",
    "        cols_bd = list(cols_map.values()) + ['fecha_creacion', 'fk_tiempo', 'latitud', 'longitud', 'geometria']\n",
    "        df_final = df[[c for c in cols_bd if c in df.columns]].copy()\n",
    "        \n",
    "        df_final.to_sql('fact_inviales', engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "        print(f\"Inviales cargados: {len(df_final)} filas.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en Inviales: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def carga_metro():\n",
    "    print(\"\\n[3/4] Procesando: Afluencia Metro...\")\n",
    "    ruta = r\"Data_Tratado\\Afluencia_Metro_Enriquecido.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(ruta, encoding='latin-1', low_memory=False)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        \n",
    "        # BÃºsqueda flexible (evita error 'fecha' vs 'Ã¯Â»Â¿fecha')\n",
    "        col_fecha = buscar_columna_flexible(df.columns, 'fecha')\n",
    "        print(f\"Columna fecha detectada: '{col_fecha}'\")\n",
    "        \n",
    "        fechas = pd.to_datetime(df[col_fecha], errors='coerce')\n",
    "        df['fecha'] = fechas\n",
    "        df['fk_tiempo'] = fechas\n",
    "        \n",
    "        df = df.rename(columns={'estacion_normalizada': 'nombre_estacion'})\n",
    "        \n",
    "        # GeometrÃ­a\n",
    "        mask = df['longitud'].notnull() & df['latitud'].notnull()\n",
    "        df.loc[mask, 'geometria'] = df[mask].apply(\n",
    "            lambda x: f\"SRID=4326;POINT({x['longitud']} {x['latitud']})\", axis=1\n",
    "        )\n",
    "        \n",
    "        cols_bd = ['fecha', 'nombre_estacion', 'linea', 'afluencia', 'latitud', 'longitud', \n",
    "                   'geometria', 'score_atlas', 'flag_historico', 'nivel_riesgo_estatico', 'fk_tiempo']\n",
    "        \n",
    "        df_final = df[[c for c in cols_bd if c in df.columns]].copy()\n",
    "        \n",
    "        df_final.to_sql('fact_afluencia_metro', engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "        print(f\"Metro cargado: {len(df_final)} filas.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en Metro: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def carga_pluviales_interpolada():\n",
    "    print(\"\\n[4/4] Procesando: Pluviales...\")\n",
    "    ruta = r\"Data_Tratado\\Pluviales_DF.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(ruta)\n",
    "        \n",
    "        df['fecha_ref'] = pd.to_datetime(\n",
    "            df['ANIO'].astype(str) + '-' + df['MES'].astype(str).str.zfill(2) + '-15'\n",
    "        )\n",
    "        \n",
    "        cols_base = ['CLAVE', 'NOMBRE', 'LON', 'LAT', 'PRECIPITACION', 'fecha_ref']\n",
    "        df = df[cols_base].sort_values('fecha_ref')\n",
    "        \n",
    "        dfs_simulados = []\n",
    "        estaciones = df['CLAVE'].unique()\n",
    "        \n",
    "        print(f\"Interpolando {len(estaciones)} estaciones...\")\n",
    "        \n",
    "        for estacion in estaciones:\n",
    "            dfe = df[df['CLAVE'] == estacion].copy()\n",
    "            dfe = dfe.drop_duplicates(subset='fecha_ref').set_index('fecha_ref')\n",
    "            \n",
    "            dfe_diario = dfe.resample('D').interpolate(method='time')\n",
    "            \n",
    "            for col in ['CLAVE', 'NOMBRE', 'LON', 'LAT']:\n",
    "                dfe_diario[col] = dfe_diario[col].ffill().bfill()\n",
    "            \n",
    "            dfe_diario = dfe_diario.reset_index().rename(columns={'fecha_ref': 'fecha'})\n",
    "            dfs_simulados.append(dfe_diario)\n",
    "            \n",
    "        df_final = pd.concat(dfs_simulados, ignore_index=True)\n",
    "        df_final = df_final.dropna(subset=['PRECIPITACION'])\n",
    "        \n",
    "        df_final = df_final.rename(columns={\n",
    "            'CLAVE': 'clave_estacion',\n",
    "            'NOMBRE': 'nombre_estacion',\n",
    "            'PRECIPITACION': 'precipitacion_simulada',\n",
    "            'LON': 'longitud',\n",
    "            'LAT': 'latitud',\n",
    "            'fecha': 'fk_tiempo'\n",
    "        })\n",
    "        df_final['fecha'] = df_final['fk_tiempo']\n",
    "        \n",
    "        df_final['geometria'] = df_final.apply(\n",
    "            lambda x: f\"SRID=4326;POINT({x['longitud']} {x['latitud']})\", axis=1\n",
    "        )\n",
    "        \n",
    "        df_final.to_sql('fact_pluviales', engine, if_exists='append', index=False, method='multi', chunksize=10000)\n",
    "        print(f\"Pluviales cargados: {len(df_final)} registros.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en Pluviales: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def finalizar_proceso():\n",
    "    print(\"\\nRefrescando Vista Maestra...\")\n",
    "    with engine.connect() as con:\n",
    "        try:\n",
    "            con.execute(text(\"REFRESH MATERIALIZED VIEW mv_hechos_con_clima;\"))\n",
    "            con.commit()\n",
    "            count = con.execute(text(\"SELECT COUNT(*) FROM mv_hechos_con_clima\")).scalar()\n",
    "            print(f\"Vista actualizada. Registros listos: {count}\")\n",
    "            print(\"\\nINGESTA TOTAL COMPLETADA.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error refrescando vista: {e}\")\n",
    "\n",
    "# EJECUCIÃ“N\n",
    "if __name__ == \"__main__\":\n",
    "    carga_hechos_transito()\n",
    "    carga_inviales()\n",
    "    carga_metro()\n",
    "    carga_pluviales_interpolada()\n",
    "    finalizar_proceso()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
